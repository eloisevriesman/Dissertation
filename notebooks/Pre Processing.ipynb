{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS FOR PARSING COMMENTS (JSON TO DATAFRAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  parse comments and replies with CommentIDs\n",
    "def parse_comments(comments, post_id, parent_id, depth, result, parent_counter=0):\n",
    "    for comment_index, comment in enumerate(comments):\n",
    "        # If it's a top-level comment, its ID is sth like post_1_0. Otherwise, it includes the parent's counter.\n",
    "        if depth == 1:  # Directly under post\n",
    "            comment_id = f\"{post_id}_{parent_counter + comment_index}\"\n",
    "        else:  # Nested comment/reply\n",
    "            comment_id = f\"{parent_id}_{comment_index + 1}\"\n",
    "\n",
    "        result.append({\n",
    "            'PostID': post_id,\n",
    "            'CommentID': comment_id,\n",
    "            'ParentID': parent_id,\n",
    "            'Author': comment['author'],\n",
    "            'Text': comment['body'],\n",
    "            'Depth': depth\n",
    "        })\n",
    "        if 'replies' in comment and comment['replies']:\n",
    "            # For replies, increment the parent counter for each new comment\n",
    "            parse_comments(comment['replies'], post_id, comment_id, depth + 1, result, 0)\n",
    "\n",
    "# process files in each subreddit directory\n",
    "def process_reddit_posts(folder_path):\n",
    "    data = []\n",
    "    post_counter = 1  \n",
    "\n",
    "    for file_path in glob.glob(os.path.join(folder_path, '*.json')):\n",
    "        with open(file_path, 'r') as file:\n",
    "            post_data = json.load(file)\n",
    "            post_id = f\"post_{post_counter}\"  # e.g., post_1\n",
    "            post_counter += 1  \n",
    "            \n",
    "            # Initially, add the post itself with a basic CommentID and no ParentID\n",
    "            data.append({\n",
    "                'PostID': post_id,\n",
    "                'CommentID': f\"{post_id}_0\",\n",
    "                'ParentID': None,\n",
    "                'Author': 'NONE',  # Placeholder\n",
    "                'Text': post_data['title'],\n",
    "                'Depth': 0\n",
    "            })\n",
    "            \n",
    "            # Process comments, starting with depth=1\n",
    "            if 'comments' in post_data:\n",
    "                parse_comments(post_data['comments'], post_id, f\"{post_id}_0\", 1, data)\n",
    "\n",
    "    return pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMBINE ALL POSTS INTO ONE DATAFRAME PER SUBREDDIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = './../data/subreddits/Scientific/ScienceUncensored_data'  # Adjust the folder path to every subreddit\n",
    "df = process_reddit_posts(folder_path)\n",
    "df.head(10)\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "df.to_csv('./../data/validation-posts/seen-subreddits/ScienceUncensored_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADDS AUTHOR TO DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './../data/labelled-posts/conservative_data.csv'  # Adjust the file path to labelled data location\n",
    "labelled_df = pd.read_csv(file_path)\n",
    "folder_path = './../data/subreddits/Political/Conservative_data' # Adjust the folder path to corresponding raw data location\n",
    "raw_df = process_reddit_posts(folder_path)\n",
    "\n",
    "# Add author & drop ParentID column\n",
    "labelled_df['Author'] = raw_df['Author']\n",
    "df = labelled_df.drop('ParentID', axis=1)\n",
    "\n",
    "csv_path = './../data/labelled-posts/conservative_data.csv' # Adjust the file path to labelled data location\n",
    "df.to_csv(file_path, index=False)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SEPARATE LABELLED AND UNLABELLED DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collects labelled comments - saving them to a csv file\n",
    "\n",
    "subreddit = 'Games' # adjust the subreddit name\n",
    "file_path = (f'./../data/validation-posts/seen-subreddits/{subreddit}_unlabelled_data.csv')\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "df = df.dropna(subset=['Combative', 'Deliberative'])\n",
    "\n",
    "csv_path = (f'./../data/{subreddit}_data.csv')\n",
    "df.to_csv(csv_path,index=False)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collects unlabelled comments - saving them to a csv file\n",
    "\n",
    "subreddit = 'Games' # adjust the subreddit name\n",
    "df = pd.read_csv(f'./../data/validation-posts/seen-subreddits/{subreddit}_unlabelled_data.csv')\n",
    "\n",
    "df = df[df['Deliberative'].isna() & df['Combative'].isna()]\n",
    "df = df.drop(columns=['Combative', 'Deliberative'])\n",
    "df = df[['PostID', 'CommentID', 'Author', 'Text','Toxicity', 'Rationality', 'Mutual Respect', 'Emotion', 'Moderator', 'Diversity'\n",
    "]]\n",
    "\n",
    "df.to_csv(f'./../data/{subreddit}_unlabelled_data.csv', index=False)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "COMBINE A FOLDER OF CSV'S INTO ONE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_files = os.path.join(\"./../data/validation-posts\", \"*.csv\") # Adjust the folder path to data location\n",
    "joined_list = glob.glob(joined_files) \n",
    "dataframes = []\n",
    "\n",
    "for file in joined_list:\n",
    "    df = pd.read_csv(file)\n",
    "    subreddit = os.path.basename(file).split('_')[0] # Adjust symbol to split the file name\n",
    "    df['Subreddit'] = subreddit\n",
    "    dataframes.append(df)\n",
    "\n",
    "\n",
    "merged_df = pd.concat(dataframes, ignore_index=True)\n",
    "column_order = ['Subreddit', 'PostID', 'CommentID', 'Author', 'Text', 'Combative', 'Deliberative', 'Toxicity', 'Rationality', 'Mutual Respect', 'Emotion', 'Moderator', 'Diversity'] # Adjust based on columns in data & desired order\n",
    "df = merged_df[column_order]\n",
    "# , 'Toxicity', 'Rationality', 'Mutual Respect', 'Emotion', 'Moderator', 'Diversity'\n",
    "\n",
    "csv_path = f'./../data/compiled-posts/validation_posts.csv' # Adjust the file path to combined data location\n",
    "df.to_csv(csv_path,index=False)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ADD A DATAFRAME TO A CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv to df\n",
    "df = pd.read_csv('./../data/validation-posts/Games_data.csv')\n",
    "\n",
    "# add a column with the subreddit name\n",
    "df['Subreddit'] = 'Games'\n",
    "\n",
    "# reorder columns\n",
    "column_order = ['Subreddit', 'PostID', 'CommentID', 'Author', 'Text', 'Toxicity', 'Rationality', 'Mutual Respect', 'Emotion', 'Moderator', 'Diversity']\n",
    "df = df[column_order]\n",
    "\n",
    "# append this dataframe to a csv file\n",
    "csv_path = './../data/compiled-posts/validation_posts_unseen copy.csv' # Adjust the file path to combined data location\n",
    "df.to_csv(csv_path, mode='a', header=False, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REORDER CSV COLUMNS TO STANDARDISE DATA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv to df\n",
    "df = pd.read_csv('./../data/unlabelled-posts/ScienceUncensored_unlabelled_data.csv')\n",
    "\n",
    "column_order = ['PostID', 'CommentID', 'Author', 'Text', 'Toxicity', 'Rationality', 'Mutual Respect', \n",
    "                'Emotion', 'Moderator', 'Diversity']\n",
    "\n",
    "# reorder columns to column_order\n",
    "df = df[column_order]\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df to csv\n",
    "df.to_csv('./../data/unlabelled-posts/ScienceUncensored_unlabelled_data.csv', index=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CLEAN DATA BY: \n",
    "- DROPPING 0-0 ANNOTATIONS (OPTIONAL)\n",
    "- MAKING SCORES > 1 EITHER 1-0 OR 0-1 (DEPENDING ON MAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../data/compiled-posts/validation_posts.csv')\n",
    "new_df = df.copy()\n",
    "\n",
    "# Max function\n",
    "def transform_values(row):\n",
    "    if row['Combative'] != row['Deliberative']:\n",
    "        if row['Combative'] > row['Deliberative']:\n",
    "            row['Combative'] = 1\n",
    "            row['Deliberative'] = 0\n",
    "        else:\n",
    "            row['Combative'] = 0\n",
    "            row['Deliberative'] = 1\n",
    "    else:\n",
    "        row['Combative'] = 0\n",
    "        row['Deliberative'] = 0\n",
    "    return row\n",
    "\n",
    "new_df = new_df.apply(transform_values, axis=1)\n",
    "\n",
    "# Drop rows where both 'Deliberative' and 'Combative' are 0\n",
    "new_df = new_df.drop(new_df[(new_df['Deliberative'] == 0) & (new_df['Combative'] == 0)].index)\n",
    "\n",
    "new_df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save new_df to csv\n",
    "new_df.to_csv('./../data/compiled-posts/validation_posts_binary.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../data/compiled-posts/validation_posts.csv')\n",
    "\n",
    "# Filter out rows where the 'Combative' and 'Deliberative' columns are equal\n",
    "filtered_df = df[df['Combative'] != df['Deliberative']]\n",
    "\n",
    "# drop combative and deliberative columns\n",
    "filtered_df = filtered_df.drop(columns=['Combative', 'Deliberative'])\n",
    "\n",
    "# Save the filtered DataFrame to a new CSV file\n",
    "output_file_path = './../data/compiled-posts/validation_data_unlabelled_binary.csv'\n",
    "filtered_df.to_csv(output_file_path, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
