{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d782c00",
   "metadata": {},
   "source": [
    "THIS FILE PERFORMS THE AUTOMATIC SCORING OF DATA FOR THE 6 METRICS - TOXICITY, RATIONALITY, MUTUAL RESSPECT, EMOTION, MODERATOR PRESENCE AND DIVERSITY."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6f9aab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-01T14:36:02.452830Z",
     "start_time": "2024-04-01T14:36:01.515679Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import glob\n",
    "from transformers import pipeline, AutoTokenizer, TFAutoModelForSequenceClassification, AutoModelForSequenceClassification\n",
    "from detoxify import Detoxify\n",
    "from flair.models import TextClassifier\n",
    "from flair.data import Sentence\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "# you want to use detoxify for toxicity and there are a couple of other libraries that also can be used for mutual respect, rationality, sentiment, diversity\n",
    "# each of these should be added as a score to the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3ac998",
   "metadata": {},
   "source": [
    "LOAD THE DATA FOR ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c042e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../data/unlabelled-posts/ScienceUncensored_unlabelled_data.csv')\n",
    "df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3c3ab7",
   "metadata": {},
   "source": [
    "TOXICITY (BATCHES FOR LARGE DATASETS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a831d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from detoxify import Detoxify\n",
    "\n",
    "# Assuming df is your DataFrame and already created\n",
    "# Example: df = pd.DataFrame({'Text': [\"sample text\"] * 10000})\n",
    "\n",
    "text_to_toxicity = df['Text'].astype(str).tolist()\n",
    "\n",
    "def process_batch(start_idx, end_idx):\n",
    "    batch_text = text_to_toxicity[start_idx:end_idx]\n",
    "    batch_scores = Detoxify('original').predict(batch_text)['toxicity']\n",
    "    return batch_scores\n",
    "\n",
    "batch_size = 500\n",
    "detoxify_scores = []\n",
    "for start_idx in range(0, len(text_to_toxicity), batch_size):\n",
    "    end_idx = min(start_idx + batch_size, len(text_to_toxicity))\n",
    "    batch_scores = process_batch(start_idx, end_idx)\n",
    "    detoxify_scores.extend(batch_scores)\n",
    "\n",
    "# Apply square root transformation\n",
    "df['Toxicity'] = detoxify_scores\n",
    "df['Toxicity'] = df['Toxicity'].apply(lambda x: x**0.5)\n",
    "\n",
    "# Display the results\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0ad247",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./../data/unlabelled-posts/ScienceUncensored_unlabelled_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77c060cf",
   "metadata": {},
   "source": [
    "TOXICITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f06d88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_to_toxicity = df['Text'].astype(str).tolist()\n",
    "detoxify_scores = Detoxify('original').predict(text_to_toxicity)['toxicity']\n",
    "# use square root transformation for toxicity score applying it onthe detoxify_scores\n",
    "df['Toxicity'] = detoxify_scores\n",
    "df['Toxicity'] = df['Toxicity'].apply(lambda x: x**0.5)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4583944f",
   "metadata": {},
   "source": [
    "RATIONALITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474c9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text, max_tokens=512):\n",
    "    tokens = tokenizer(text, truncation=True, max_length=max_tokens, return_tensors=\"pt\")\n",
    "    truncated_text = tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
    "    return truncated_text\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/bias-detection-model\", max_length=512, padding=True, truncation=True)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"d4data/bias-detection-model\")\n",
    "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer) # cuda = 0,1 based on gpu availability\n",
    "\n",
    "df['Truncated Text'] = df['Text'].apply(lambda x: truncate_text(x, max_tokens=512))\n",
    "df['Rationality'] = classifier(df['Truncated Text'].tolist())\n",
    "df = df.drop(columns=['Truncated Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b19778",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_rationality(row):\n",
    "    rationality = row['Rationality']\n",
    "    \n",
    "    if isinstance(rationality, dict):\n",
    "        label = rationality['label']\n",
    "        score = rationality['score']\n",
    "\n",
    "        if label == 'Non-biased':\n",
    "            return score\n",
    "        else:\n",
    "            return 1 - score\n",
    "    else:\n",
    "        return row['Rationality']\n",
    "    \n",
    "df['Rationality'] = df.apply(modify_rationality, axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc29cdc7",
   "metadata": {},
   "source": [
    "MUTUAL RESPECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75fc8ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate_text(text, max_tokens=512):\n",
    "    tokens = tokenizer(text, truncation=True, max_length=max_tokens, return_tensors=\"pt\")\n",
    "    truncated_text = tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
    "    return truncated_text\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NOVA-vision-language/polite_bert\")\n",
    "respect_model = AutoModelForSequenceClassification.from_pretrained(\"NOVA-vision-language/polite_bert\")\n",
    "classifier = pipeline('text-classification', model=respect_model, tokenizer=tokenizer)\n",
    "\n",
    "df['Truncated Text'] = df['Text'].apply(lambda x: truncate_text(x, max_tokens=tokenizer.model_max_length))\n",
    "df['Mutual Respect'] = classifier(df['Truncated Text'].tolist())\n",
    "df = df.drop(columns=['Truncated Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6fa40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_mutual_respect(row):\n",
    "    mutual_respect = row['Mutual Respect']\n",
    "    \n",
    "    if isinstance(mutual_respect, dict):\n",
    "        label = mutual_respect['label']\n",
    "        score = mutual_respect['score']\n",
    "\n",
    "        if label == 'POLITE':\n",
    "            return score\n",
    "        elif label == 'SOMEWHAT_POLITE':\n",
    "            return score * 0.7\n",
    "        elif label == 'NEUTRAL':\n",
    "            return score * 0.5\n",
    "        else:\n",
    "            return 1 - score\n",
    "    else:\n",
    "        return row['Mutual Respect']\n",
    "\n",
    "df['Mutual Respect'] = df.apply(modify_mutual_respect, axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ee19041",
   "metadata": {},
   "source": [
    "EMOTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60806d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = TextClassifier.load('en-sentiment')\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    sentence = Sentence(text)\n",
    "    classifier.predict(sentence)\n",
    "    return sentence.labels[0].score\n",
    "\n",
    "df['Emotion'] = df['Text'].apply(predict_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e142fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use flair to predict the emotion score of the text\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "def predict_sentiment(text):\n",
    "    sentence = Sentence(text)\n",
    "    classifier.predict(sentence)\n",
    "    # if the value is NEGATIVE then we take - value if the value is POSITIVE then we just keep that value\n",
    "    if sentence.labels[0].value == 'NEGATIVE':\n",
    "        return sentence.labels[0].score* -1\n",
    "    else:\n",
    "        return sentence.labels[0].score\n",
    "\n",
    "emotion = df['Text'].apply(predict_sentiment)\n",
    "scaled_emotion = (emotion+1)/2 # scale the emotion scores to be between 0 and 1\n",
    "df['Emotion'] = scaled_emotion\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679edccd",
   "metadata": {},
   "source": [
    "MODERATOR PRESENCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be14974",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./../data/utils/moderators.json', 'r') as f:  # ensure the path to json of moderators for all subreddits is correct\n",
    "    moderators_dict = json.load(f)\n",
    "\n",
    "def is_moderator(author, subreddit='formula1'): # ensure subreddit is the correct subreddit\n",
    "    return 1 if author in moderators_dict.get(subreddit, []) or author == 'AutoModerator' else 0\n",
    "\n",
    "df['Moderator'] = df['Author'].apply(lambda x: is_moderator(x))\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447414e5",
   "metadata": {},
   "source": [
    "DIVERSITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3c1d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "author_post_pairs = set()\n",
    "\n",
    "def is_first_comment(post_id, author):\n",
    "    pair = (post_id, author)\n",
    "    if pair in author_post_pairs:\n",
    "        return 0\n",
    "    else:\n",
    "        author_post_pairs.add(pair)\n",
    "        return 1\n",
    "\n",
    "df['Diversity'] = df.apply(lambda row: is_first_comment(row['PostID'], row['Author']), axis=1)\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb5ae20",
   "metadata": {},
   "source": [
    "SAVE DATAFRAME TO CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00484d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust path to the correct subreddit name & location\n",
    "df.to_csv('./../data/unlabelled-posts/fauxmoi_unlabelled_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fcbc1c",
   "metadata": {},
   "source": [
    "SINGLE FUNCTION TO SCORE POSTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc3a95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../data/validation-posts/science_data.csv')\n",
    "df.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a895c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toxicity Scoring\n",
    "\n",
    "text_to_toxicity = df['Text'].astype(str).tolist()\n",
    "detoxify_scores = Detoxify('original').predict(text_to_toxicity)['toxicity']\n",
    "df['Toxicity'] = detoxify_scores\n",
    "df['Toxicity'] = df['Toxicity'].apply(lambda x: x**0.5)\n",
    "\n",
    "# Rationality Scoring\n",
    "\n",
    "def truncate_text(text, max_tokens=512):\n",
    "    tokens = tokenizer(text, truncation=True, max_length=max_tokens, return_tensors=\"pt\")\n",
    "    truncated_text = tokenizer.decode(tokens['input_ids'][0], skip_special_tokens=True)\n",
    "    return truncated_text\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"d4data/bias-detection-model\", max_length=512, padding=True, truncation=True)\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"d4data/bias-detection-model\")\n",
    "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer) # cuda = 0,1 based on gpu availability\n",
    "\n",
    "df['Truncated Text'] = df['Text'].apply(lambda x: truncate_text(x, max_tokens=512))\n",
    "df['Rationality'] = classifier(df['Truncated Text'].tolist())\n",
    "\n",
    "def modify_rationality(row):\n",
    "    rationality = row['Rationality']\n",
    "    \n",
    "    if isinstance(rationality, dict):\n",
    "        label = rationality['label']\n",
    "        score = rationality['score']\n",
    "\n",
    "        if label == 'Non-biased':\n",
    "            return score\n",
    "        else:\n",
    "            return 1 - score\n",
    "    else:\n",
    "        return row['Rationality']\n",
    "    \n",
    "df['Rationality'] = df.apply(modify_rationality, axis=1)\n",
    "\n",
    "# Mutual Respect Scoring\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"NOVA-vision-language/polite_bert\")\n",
    "respect_model = AutoModelForSequenceClassification.from_pretrained(\"NOVA-vision-language/polite_bert\")\n",
    "classifier = pipeline('text-classification', model=respect_model, tokenizer=tokenizer)\n",
    "\n",
    "df['Mutual Respect'] = classifier(df['Truncated Text'].tolist())\n",
    "df = df.drop(columns=['Truncated Text'])\n",
    "\n",
    "def modify_mutual_respect(row):\n",
    "    mutual_respect = row['Mutual Respect']\n",
    "    \n",
    "    if isinstance(mutual_respect, dict):\n",
    "        label = mutual_respect['label']\n",
    "        score = mutual_respect['score']\n",
    "\n",
    "        if label == 'POLITE':\n",
    "            return score\n",
    "        elif label == 'SOMEWHAT_POLITE':\n",
    "            return score * 0.7\n",
    "        elif label == 'NEUTRAL':\n",
    "            return score * 0.5\n",
    "        else:\n",
    "            return 1 - score\n",
    "    else:\n",
    "        return row['Mutual Respect']\n",
    "\n",
    "df['Mutual Respect'] = df.apply(modify_mutual_respect, axis=1)\n",
    "\n",
    "# Emotion Scoring\n",
    "\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "\n",
    "def predict_sentiment(text):\n",
    "    sentence = Sentence(text)\n",
    "    classifier.predict(sentence)\n",
    "    return sentence.labels[0].score\n",
    "\n",
    "df['Emotion'] = df['Text'].apply(predict_sentiment)\n",
    "\n",
    "classifier = TextClassifier.load('en-sentiment')\n",
    "def predict_sentiment(text):\n",
    "    sentence = Sentence(text)\n",
    "    classifier.predict(sentence)\n",
    "    if sentence.labels[0].value == 'NEGATIVE':\n",
    "        return sentence.labels[0].score* -1\n",
    "    else:\n",
    "        return sentence.labels[0].score\n",
    "\n",
    "emotion = df['Text'].apply(predict_sentiment)\n",
    "scaled_emotion = (emotion+1)/2 \n",
    "df['Emotion'] = scaled_emotion\n",
    "\n",
    "# Moderator Scoring\n",
    "\n",
    "with open('./../data/utils/moderators.json', 'r') as f:  # ensure the path to json of moderators for all subreddits is correct\n",
    "    moderators_dict = json.load(f)\n",
    "\n",
    "\"\"\"def is_moderator(author, subreddit): # ensure subreddit is the correct subreddit\n",
    "    return 1 if author in moderators_dict.get(subreddit, []) else 0\n",
    "\n",
    "df['Moderator'] = df.apply(lambda row: is_moderator(row['Author'], row['Subreddit']), axis=1)\"\"\"\n",
    "\n",
    "def is_moderator(author, subreddit='science'): # ensure subreddit is the correct subreddit\n",
    "    return 1 if author in moderators_dict.get(subreddit, []) else 0\n",
    "\n",
    "df['Moderator'] = df['Author'].apply(lambda x: is_moderator(x))\n",
    "df.head(10)\n",
    "\n",
    "# Diversity Scoring\n",
    "\n",
    "author_post_pairs = set()\n",
    "\n",
    "def is_first_comment(post_id, author):\n",
    "    pair = (post_id, author)\n",
    "    if pair in author_post_pairs:\n",
    "        return 0\n",
    "    else:\n",
    "        author_post_pairs.add(pair)\n",
    "        return 1\n",
    "\n",
    "df['Diversity'] = df.apply(lambda row: is_first_comment(row['PostID'], row['Author']), axis=1)\n",
    "\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac02977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust path to the correct subreddit name & location\n",
    "df.to_csv('./../data/validation-posts/science_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
